{"version":3,"file":"content.js","mappings":";;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gC","sources":["webpack://empty-project/./src/index.js"],"sourcesContent":["//console.log(\"content.js loaded\");\n//class AIExtension{\n//    constructor(){\n//        this.handleRequest();\n//    }\n//    handleRequest(){\n//        chrome.runtime.onMessage.addListener( async (request, sender, response) => {\n//            if(request.action == \"PROMPT\"){\n//                this.promptoai(request.message);\n//                console.log(request.message);\n//            }\n//        })\n//    }\n//    promptoai(prompt){\n//        console.log(\"we are prompting to AI\");\n//        fetch(\"http://localhost:11434/api/generate\", {\n//              method: \"POST\",\n//              headers: { \"Content-Type\": \"application/json\" },\n//              body: JSON.stringify({\n//                model: \"deepseek-r1:8b\",\n//                prompt: prompt\n//              })\n//            })\n//              .then(async (res) => {\n//                const text = await res.text(); // Ollama streams text, not always JSON\n//                console.log(\"Response: \");\n//                //sendResponse({ success: true, data: text });\n//              })\n//              .catch((err) => {\n//                console.error(\"DeepSeek fetch error:\", err);\n//                //sendResponse({ success: false, error: err.message });\n//              });\n//\n//            return true; // âœ… keep channel open for async\n//    }\n//}\n//const aie = new AIExtension();\n"],"names":[],"sourceRoot":""}